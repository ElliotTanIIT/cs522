{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CS522-GPT2.ipynb","provenance":[],"collapsed_sections":["VhkqZQ7nimGx","Cy90xYAmLXrH","oR7TKwRbOFmJ","yXdO4P1pifVb","oXkeXv3_PDDQ","4jGuTR1sft-N","QiFFTlcmf3Ye","FUuQBvQOf-rP"],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"naLrQ9pb2Xyq"},"source":["# SVM"]},{"cell_type":"markdown","metadata":{"id":"VhkqZQ7nimGx"},"source":["# GPT2"]},{"cell_type":"code","metadata":{"id":"LQp15RXlleak","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617905368043,"user_tz":300,"elapsed":3305,"user":{"displayName":"Alexis Edwards","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiBybh3MHhSTM4QYO0umj0CWSJNLLaIbispEle2=s64","userId":"05307814282650774487"}},"outputId":"65f69e56-d734-4036-bde4-db6e6eccb78e"},"source":["! pip install tensorflow"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.4.1)\n","Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.7.4.3)\n","Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.4.1)\n","Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.19.5)\n","Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.36.2)\n","Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12)\n","Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.10.0)\n","Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.12.4)\n","Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12.1)\n","Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.32.0)\n","Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n","Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.4.0)\n","Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n","Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.12.0)\n","Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n","Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n","Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.3.3)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (1.8.0)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (2.23.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (0.4.3)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (1.28.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (54.2.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (1.0.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (3.3.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2020.12.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (3.0.4)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (1.3.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.7.2)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.2.1)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow) (3.8.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (3.1.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.4.8)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow) (3.4.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"aUpVHoJMmrGo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617905371282,"user_tz":300,"elapsed":3525,"user":{"displayName":"Alexis Edwards","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiBybh3MHhSTM4QYO0umj0CWSJNLLaIbispEle2=s64","userId":"05307814282650774487"}},"outputId":"001d5b6f-cd04-44f0-84c3-00a7fb07f8d6"},"source":["! pip install transformers"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.5.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.44)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.2)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5pzqHvd3oA0B"},"source":["#Import package\n","from transformers import pipeline, set_seed"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b7r66nTfosDW"},"source":["#Initialize pipeline\n","generator = pipeline('text-generation', model='gpt2')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HJjlHZrQox5F","executionInfo":{"status":"ok","timestamp":1617226819561,"user_tz":300,"elapsed":3457,"user":{"displayName":"Alexis Edwards","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiBybh3MHhSTM4QYO0umj0CWSJNLLaIbispEle2=s64","userId":"05307814282650774487"}},"outputId":"e068e6fc-4a14-4a73-d958-986179f209ae"},"source":["#Test case using generator\n","generator(\"Hello, I am a language model,\", max_length=30, num_return_sequences=5)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["[{'generated_text': 'Hello, I am a language model, so I guess there goes the matter. So your first answer is about languages in general, and the way we'},\n"," {'generated_text': \"Hello, I am a language model, right? No, it's not. To keep the language interesting, I wanted to create, for every event\"},\n"," {'generated_text': 'Hello, I am a language model, I have an idea for writing a software application that\\'s not limited to a particular region of that language.\"\\n'},\n"," {'generated_text': \"Hello, I am a language model, but I can't change it. I want to be comfortable with what my language is and how it compares to\"},\n"," {'generated_text': 'Hello, I am a language model, this one is about to get much more complex!\" Well, because I\\'m going to do something totally different:'}]"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"bxE4cDZSo-Og","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617048719684,"user_tz":300,"elapsed":6489,"user":{"displayName":"Elliot Tan","photoUrl":"","userId":"13442172689223781275"}},"outputId":"7f46e67e-a74c-4656-c832-b2c36f771b57"},"source":["generator(\"Hello, I am a language model,\", max_length=60, num_return_sequences=5)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["[{'generated_text': 'Hello, I am a language model, not a system.\"\\n\\n\"No, it\\'s not!\" she protested. \"I want to learn. I know you\\'re not going to be a language learner. I don\\'t know how you can say this...but your problem isn\\'t new at'},\n"," {'generated_text': \"Hello, I am a language model, but, you know, I'm a big-time programmer. There's a lot of cool languages, and I would love to work at a great company, like IBM. So, in this interview, I talked to you about Python and C.\\n\\n\"},\n"," {'generated_text': \"Hello, I am a language model, not a user interface. Why should you need to use a language model for this? If so, what? I don't know.\\n\\nMy question is, why should you choose a good language model for a text game, and then have it use a\"},\n"," {'generated_text': \"Hello, I am a language model, and this is something that is not easy to put down. I think if programmers can figure out that this isn't necessary, why not take some time to figure out which way in which programming is going?\\n\\nThis sounds somewhat of a philosophical objection, because\"},\n"," {'generated_text': 'Hello, I am a language model, you know. I want you to develop. My dream is for you to have a career in one of three fields: computer science, mathematics, engineering and mathematics. After a few years at one of these two areas, your field will become even more specialized.'}]"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"B0EMN7rgD69q"},"source":["# Hate Speech dataframe"]},{"cell_type":"code","metadata":{"id":"MWSmr-DkE0hQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617297408642,"user_tz":300,"elapsed":31841,"user":{"displayName":"Elliot Tan","photoUrl":"","userId":"13442172689223781275"}},"outputId":"e99dfe65-c6b4-448d-85d5-03de5e7a6b2a"},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9Ya-luphFdCK","colab":{"base_uri":"https://localhost:8080/","height":402},"executionInfo":{"status":"ok","timestamp":1617311909718,"user_tz":300,"elapsed":302,"user":{"displayName":"Elliot Tan","photoUrl":"","userId":"13442172689223781275"}},"outputId":"c78ee3b1-a31a-4caa-b714-ceca43d5af15"},"source":["# Load data from CSV file\n","\n","from google.colab import files\n","import pandas as pd\n","import io\n","# uploaded = files.upload()\n","#df = pd.read_csv(\"/content/hate_speech_labeled_data.csv\")\n","df = pd.read_csv(\"/content/drive/Shareddrives/CS522 term project - bullying hatespeech/hate_speech_labeled_data.csv\")\n","\n","# get the tweet column\n","df\n","\n","# Get the first five rows\n","# df.head()\n","\n","# Get the last 5 rows\n","# df.tail()\n","\n","# Other potentially useful functions\n","# df.shape\n","# df.size\n","# df.count()\n","# df['class'].value_counts()\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>count</th>\n","      <th>hate_speech</th>\n","      <th>offensive_language</th>\n","      <th>neither</th>\n","      <th>class</th>\n","      <th>tweet</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>24778</th>\n","      <td>25291</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>you's a muthaf***in lie &amp;#8220;@LifeAsKing: @2...</td>\n","    </tr>\n","    <tr>\n","      <th>24779</th>\n","      <td>25292</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>you've gone and broke the wrong heart baby, an...</td>\n","    </tr>\n","    <tr>\n","      <th>24780</th>\n","      <td>25294</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>young buck wanna eat!!.. dat nigguh like I ain...</td>\n","    </tr>\n","    <tr>\n","      <th>24781</th>\n","      <td>25295</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>youu got wild bitches tellin you lies</td>\n","    </tr>\n","    <tr>\n","      <th>24782</th>\n","      <td>25296</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>~~Ruffled | Ntac Eileen Dahlia - Beautiful col...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>24783 rows × 7 columns</p>\n","</div>"],"text/plain":["       Unnamed: 0  ...                                              tweet\n","0               0  ...  !!! RT @mayasolovely: As a woman you shouldn't...\n","1               1  ...  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...\n","2               2  ...  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...\n","3               3  ...  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...\n","4               4  ...  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...\n","...           ...  ...                                                ...\n","24778       25291  ...  you's a muthaf***in lie &#8220;@LifeAsKing: @2...\n","24779       25292  ...  you've gone and broke the wrong heart baby, an...\n","24780       25294  ...  young buck wanna eat!!.. dat nigguh like I ain...\n","24781       25295  ...              youu got wild bitches tellin you lies\n","24782       25296  ...  ~~Ruffled | Ntac Eileen Dahlia - Beautiful col...\n","\n","[24783 rows x 7 columns]"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":606},"id":"3-WcE7Po70le","executionInfo":{"status":"ok","timestamp":1617289657066,"user_tz":300,"elapsed":190,"user":{"displayName":"Alexis Edwards","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiBybh3MHhSTM4QYO0umj0CWSJNLLaIbispEle2=s64","userId":"05307814282650774487"}},"outputId":"c3973fb3-1440-460b-c905-af56342dfe9c"},"source":["# Distribution of the classes\n","\n","# Hate Speech (200 Records out of 19,190 Total)\n","hs_df = df[df['class']==0][0:200]\n","\n","# Offensive Language (200 Records out of 2,163 Total)\n","ol_df = df[df['class']==1][0:200]\n","\n","# Neither (200 Records out of 1,430 Total)\n","neither_df = df[df['class']==2][0:200]\n","\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>count</th>\n","      <th>hate_speech</th>\n","      <th>offensive_language</th>\n","      <th>neither</th>\n","      <th>class</th>\n","      <th>tweet</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n","    </tr>\n","    <tr>\n","      <th>40</th>\n","      <td>40</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>\" momma said no pussy cats inside my doghouse \"</td>\n","    </tr>\n","    <tr>\n","      <th>63</th>\n","      <td>63</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>\"@Addicted2Guys: -SimplyAddictedToGuys http://...</td>\n","    </tr>\n","    <tr>\n","      <th>66</th>\n","      <td>66</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>\"@AllAboutManFeet: http://t.co/3gzUpfuMev\" woo...</td>\n","    </tr>\n","    <tr>\n","      <th>67</th>\n","      <td>67</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>\"@Allyhaaaaa: Lemmie eat a Oreo &amp;amp; do these...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1113</th>\n","      <td>1137</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>&amp;#8220;@AshlynWynns: I'm a \"mud shark\" because...</td>\n","    </tr>\n","    <tr>\n","      <th>1114</th>\n","      <td>1138</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>&amp;#8220;@Ashton5SOS: An itsy bitsy teeny weeny ...</td>\n","    </tr>\n","    <tr>\n","      <th>1116</th>\n","      <td>1141</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>&amp;#8220;@AustinMahone: just got a new fish it's...</td>\n","    </tr>\n","    <tr>\n","      <th>1128</th>\n","      <td>1153</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>&amp;#8220;@BasebaIIVines: Michael Pineda hides pi...</td>\n","    </tr>\n","    <tr>\n","      <th>1134</th>\n","      <td>1159</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>&amp;#8220;@BeautyOfASinner: So they really delete...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>200 rows × 7 columns</p>\n","</div>"],"text/plain":["      Unnamed: 0  ...                                              tweet\n","0              0  ...  !!! RT @mayasolovely: As a woman you shouldn't...\n","40            40  ...    \" momma said no pussy cats inside my doghouse \"\n","63            63  ...  \"@Addicted2Guys: -SimplyAddictedToGuys http://...\n","66            66  ...  \"@AllAboutManFeet: http://t.co/3gzUpfuMev\" woo...\n","67            67  ...  \"@Allyhaaaaa: Lemmie eat a Oreo &amp; do these...\n","...          ...  ...                                                ...\n","1113        1137  ...  &#8220;@AshlynWynns: I'm a \"mud shark\" because...\n","1114        1138  ...  &#8220;@Ashton5SOS: An itsy bitsy teeny weeny ...\n","1116        1141  ...  &#8220;@AustinMahone: just got a new fish it's...\n","1128        1153  ...  &#8220;@BasebaIIVines: Michael Pineda hides pi...\n","1134        1159  ...  &#8220;@BeautyOfASinner: So they really delete...\n","\n","[200 rows x 7 columns]"]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"code","metadata":{"id":"alJgInyHASG3"},"source":["# Divide data as Test/Train set\n","from sklearn.model_selection import train_test_split\n","\n","train_X, test_X, train_Y, test_Y = train_test_split(df['tweet'], df['class'], test_size = 0.2)\n","\n","\n","# Encoding\n","from sklearn.preprocessing import LabelEncoder\n","\n","Encoder = LabelEncoder()\n","train_Y = Encoder.fit_transform(train_Y)\n","test_Y = Encoder.fit_transform(test_Y)\n","\n","# Word Vectorization\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","Tfidf_vector = TfidfVectorizer(max_features=5000)\n","Tfidf_vector.fit(df['tweet'])\n","\n","train_X_tfidf = Tfidf_vector.transform(train_X)\n","test_X_tfidf = Tfidf_vector.transform(test_X)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tfu9B-3kCtf5","executionInfo":{"status":"ok","timestamp":1617291693095,"user_tz":300,"elapsed":105000,"user":{"displayName":"Alexis Edwards","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiBybh3MHhSTM4QYO0umj0CWSJNLLaIbispEle2=s64","userId":"05307814282650774487"}},"outputId":"11a5e8aa-ec83-44b7-f5cb-3d4e39cea567"},"source":["# SVM\n","\n","# Import Model\n","import numpy as np\n","from sklearn import svm\n","\n","# Create a classifier\n","clf = svm.SVC()\n","\n","# Train the model\n","clf.fit(train_X_tfidf, train_Y)\n","\n","# Predict labels on test data\n","predictions = clf.predict(test_X_tfidf)\n","\n","# Get the accuracy\n","from sklearn.metrics import accuracy_score\n","\n","print(\"SVM Accuracy Score -> \", accuracy_score(predictions, test_Y)*100)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["SVM Accuracy Score ->  90.05446842848497\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Cy90xYAmLXrH"},"source":["# Words to Ignore"]},{"cell_type":"code","metadata":{"id":"cgiHPEuXLbfE"},"source":["def ignore_word(word):\n","  if word.startswith('@', 0, 1):\n","    return True\n","  else: \n","    return False"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oR7TKwRbOFmJ"},"source":["# Lemmatization using WordNetLemmatizer\n","### Alexis Edwards\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UBz2oWp2Knod","executionInfo":{"status":"ok","timestamp":1617227019869,"user_tz":300,"elapsed":2057,"user":{"displayName":"Alexis Edwards","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiBybh3MHhSTM4QYO0umj0CWSJNLLaIbispEle2=s64","userId":"05307814282650774487"}},"outputId":"549f2ccf-f0b0-424e-f9ce-3a065b69a5ea"},"source":["#Import library\n","import nltk\n","nltk.download('wordnet')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"zsq9X7EEJ3jQ"},"source":["#Import package\n","from nltk.stem import WordNetLemmatizer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YWFw8CUMKKWn"},"source":["#Instantiate Lemmatizer\n","lemmatizer = WordNetLemmatizer()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XxFd9qTfPU3C","executionInfo":{"status":"ok","timestamp":1617227037509,"user_tz":300,"elapsed":498,"user":{"displayName":"Alexis Edwards","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiBybh3MHhSTM4QYO0umj0CWSJNLLaIbispEle2=s64","userId":"05307814282650774487"}},"outputId":"15865655-ef3d-4c60-c372-5b6d8bd8f3e5"},"source":["#Use examples\n","\n","text1 = \"cats\"\n","print(lemmatizer.lemmatize(text1))\n","\n","text1 = \"better\"\n","print(lemmatizer.lemmatize(text1, 'a'))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["cat\n","good\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":232},"id":"ndFdlsGEKVv4","executionInfo":{"status":"error","timestamp":1617226731399,"user_tz":300,"elapsed":919,"user":{"displayName":"Alexis Edwards","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiBybh3MHhSTM4QYO0umj0CWSJNLLaIbispEle2=s64","userId":"05307814282650774487"}},"outputId":"851fd9f7-583a-49f4-fc06-dec0ddebdbb8"},"source":["#Apply to data\n","for tweet in df['tweet']:\n","  lemmatizer.lemmatize(tweet)\n","\n","df['tweet']"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-077454c273aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Practice Examples:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#Second paramater = Part of speech (Default is 'n')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"yXdO4P1pifVb"},"source":["# Removing rare words\n","### Alexis Edwards"]},{"cell_type":"code","metadata":{"id":"CiOXdP3VP6c5"},"source":["# Import package\n","import nltk"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LOOFgSJCQC8H"},"source":["#Setting up example\n","tokens = ['hi','i','am','am','whatever','this','is','just','a','test','test','java','python','java']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L0DezKX4QID4","executionInfo":{"status":"ok","timestamp":1617228994243,"user_tz":300,"elapsed":428,"user":{"displayName":"Alexis Edwards","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiBybh3MHhSTM4QYO0umj0CWSJNLLaIbispEle2=s64","userId":"05307814282650774487"}},"outputId":"ccfbef2d-1e1c-44fb-e0e1-5dfef6b25ebe"},"source":["#During tokenization, need to update a list of all words\n","\n","# Array of all words found in the data\n","tokens = []\n","\n","# Calculate frequency of each token\n","freq_dist = nltk.FreqDist(tokens)\n","\n","# rarewords contains all words that occur once (AKA rare/made up)\n","rarewords = freq_dist.hapaxes()\n","\n","#Helper function (Credit to GeeksforGeeks)\n","def Diff(li1, li2):\n","    return (list(list(set(li1)-set(li2)) + list(set(li2)-set(li1))))\n","\n","# Keep only non-rare words:\n","for tweet in df['tweet']:\n","  for word in tweet:\n","    if word in rarewords:\n","      #ignore word (Not sure how best to do this)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['hi', 'i', 'whatever', 'this', 'is', 'just', 'a', 'python']\n","['java', 'am', 'test']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"oXkeXv3_PDDQ"},"source":["# Replacing Emojis with Words\n","### Alexis Edwards\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S0tQvXm_P0dL","executionInfo":{"status":"ok","timestamp":1617146080513,"user_tz":300,"elapsed":3366,"user":{"displayName":"Alexis Edwards","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiBybh3MHhSTM4QYO0umj0CWSJNLLaIbispEle2=s64","userId":"05307814282650774487"}},"outputId":"719af642-dedb-4795-a50e-c70e5865bd51"},"source":["#Download libray\n","! pip install emoji --upgrade"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already up-to-date: emoji in /usr/local/lib/python3.7/dist-packages (1.2.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_Jy0j4vnPHLO"},"source":["#Import package\n","import emoji"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zMJXsYHvQzzd","executionInfo":{"status":"ok","timestamp":1617146091571,"user_tz":300,"elapsed":210,"user":{"displayName":"Alexis Edwards","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiBybh3MHhSTM4QYO0umj0CWSJNLLaIbispEle2=s64","userId":"05307814282650774487"}},"outputId":"30c68b24-9a31-445a-991a-d14611c3356f"},"source":["#Example usage:\n","text = \"Hilarious 😂\"\n","print(emoji.demojize(text, delimiters=(\"\", \"\")))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Hilarious face_with_tears_of_joy\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IavEYeBfUaea"},"source":["#Apply to data\n","\n","for tweet in df['tweet']:\n","  emoji.demojize(tweet, delimiters=(\"\", \"\"))\n","\n","df['tweet']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4jGuTR1sft-N"},"source":["#Remove Stopwords\n","###Akhilesh Datar"]},{"cell_type":"code","metadata":{"id":"ifxwNECghqwz"},"source":["!pip install -q wordcloud\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"84qqCHGSh7co","executionInfo":{"status":"ok","timestamp":1617131226375,"user_tz":300,"elapsed":767,"user":{"displayName":"Akhilesh Datar","photoUrl":"","userId":"01956764686687934981"}},"outputId":"97272b6c-6344-499b-d168-a73fb5096a5f"},"source":["import nltk\n","nltk.download('stopwords')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"UiLWCixhRG2X"},"source":["#Define Remove Stop Words functions (Credit to: https://towardsdatascience.com/text-preprocessing-for-data-scientist-3d2419c8199d)\n","\n","#Importing stopwords from nltk library\n","from nltk.corpus import stopwords\n","STOPWORDS = set(stopwords.words('english'))\n","\n","# Function to remove the stopwords\n","def stopwords(text):\n","    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"k_hzzXkpqtHc","executionInfo":{"status":"ok","timestamp":1617133522799,"user_tz":300,"elapsed":516,"user":{"displayName":"Akhilesh Datar","photoUrl":"","userId":"01956764686687934981"}},"outputId":"18b9e1ad-3555-4789-ffc2-a2da2c9b37a9"},"source":["ef = \"\"\"This is a sample sentence,\n","                  showing off the stop words filtration.\"\"\"\n","stopwords(ef)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'This sample sentence, showing stop words filtration.'"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"id":"wKV-hsEQiMGw","executionInfo":{"status":"ok","timestamp":1617131453020,"user_tz":300,"elapsed":869,"user":{"displayName":"Akhilesh Datar","photoUrl":"","userId":"01956764686687934981"}},"outputId":"fb575158-6b39-4f91-80bd-7d67ce955c3e"},"source":["import pandas as pd\n","\n","\n","\"\"\"df = pd.read_csv(\"sample.csv\")\n","df.head()\n","\n","\n","# Applying the stopwords to 'text_punct' and store into 'text_stop'\n","df[\"text_stop\"] = df[\"text_punct\"].apply(stopwords)\n","df[\"text_stop\"].head()\"\"\""],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'df = pd.read_csv(\"sample.csv\")\\ndf.head()\\n\\n\\n# Applying the stopwords to \\'text_punct\\' and store into \\'text_stop\\'\\ndf[\"text_stop\"] = df[\"text_punct\"].apply(stopwords)\\ndf[\"text_stop\"].head()'"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"QiFFTlcmf3Ye"},"source":["#Transform numbers to words  \n","###Akhilesh Datar"]},{"cell_type":"code","metadata":{"id":"Gup5QaGQfyjz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617133840160,"user_tz":300,"elapsed":3944,"user":{"displayName":"Akhilesh Datar","photoUrl":"","userId":"01956764686687934981"}},"outputId":"c219dc27-8fb4-469a-b2c3-a15f5b93edb0"},"source":["pip install num2words"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting num2words\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/eb/a2/ea800689730732e27711c41beed4b2a129b34974435bdc450377ec407738/num2words-0.5.10-py3-none-any.whl (101kB)\n","\r\u001b[K     |███▎                            | 10kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 20kB 11.0MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 30kB 14.4MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 40kB 9.5MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 51kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 61kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 71kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 81kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 92kB 5.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 102kB 3.6MB/s \n","\u001b[?25hRequirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.7/dist-packages (from num2words) (0.6.2)\n","Installing collected packages: num2words\n","Successfully installed num2words-0.5.10\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7gBuYBL1sL4H"},"source":["from num2words import num2words"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_xUUKI73sEhD","executionInfo":{"status":"ok","timestamp":1617134038422,"user_tz":300,"elapsed":981,"user":{"displayName":"Akhilesh Datar","photoUrl":"","userId":"01956764686687934981"}},"outputId":"a7971ad6-b5d8-460e-8f14-c860a60b2c20"},"source":["from num2words import num2words\n","  \n","# Most common usage.\n","print(num2words(36))\n","  \n","# Other variants, according to the type of article.\n","print(num2words(2021, to = 'ordinal'))\n","print(num2words(2021, to = 'ordinal_num'))\n","print(num2words(2021, to = 'year'))\n","print(num2words(2021, to = 'currency'))\n","  \n","# Language Support.\n","print(num2words(2021, lang ='en_IN'))\n","print(num2words(2021, lang ='ru'))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["thirty-six\n","two thousand and twenty-first\n","2021st\n","twenty twenty-one\n","twenty euro, twenty-one cents\n","two thousand and twenty-one\n","две тысячи двадцать один\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FUuQBvQOf-rP"},"source":["#Stemming\n","###Akhilesh Datar"]},{"cell_type":"code","metadata":{"id":"_i79mZOGgE23","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617132256870,"user_tz":300,"elapsed":1505,"user":{"displayName":"Akhilesh Datar","photoUrl":"","userId":"01956764686687934981"}},"outputId":"88ab1fd8-2f33-48ca-ac22-c2450c5fd747"},"source":["from nltk.stem import PorterStemmer\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","\n","nltk.download('punkt')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"4t0vWXWgkMFw"},"source":["#Source from https://www.guru99.com/stemming-lemmatization-python-nltk.html\n","\n","#words = word_tokenize(sentence)\n","\n","\n","def Stem_this(block_of_words):\n","  if type(block_of_words) is list: #This is for lists\n","    ps =PorterStemmer()\n","    for w in block_of_words:\n","      rootWord=ps.stem(w)\n","      print(rootWord)\n","  else:                           #This is non list stuff\n","    words = word_tokenize(block_of_words)\n","    ps = PorterStemmer()\n","    for w in words:\n","      rootWord=ps.stem(w)\n","      print(rootWord)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RoRFRUESlK19","executionInfo":{"status":"ok","timestamp":1617132709785,"user_tz":300,"elapsed":560,"user":{"displayName":"Akhilesh Datar","photoUrl":"","userId":"01956764686687934981"}},"outputId":"7d951ec4-20ee-436e-bf88-441247c4079a"},"source":["sentence= \"So this is my attempt at Stemming these words. Its simple its not a bunch of jargon.\"\n","Stem_this(sentence)\n","print(\"\\n\")\n","listy = [\"wait\", \"waiting\", \"waited\", \"waits\"]\n","Stem_this(listy)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["So\n","thi\n","is\n","my\n","attempt\n","at\n","stem\n","these\n","word\n",".\n","it\n","simpl\n","it\n","not\n","a\n","bunch\n","of\n","jargon\n",".\n","\n","\n","wait\n","wait\n","wait\n","wait\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"58US4ajS8en9"},"source":["# Correcting Typos\n","### Elliot"]},{"cell_type":"code","metadata":{"id":"k33OrYeqljTH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617297354838,"user_tz":300,"elapsed":4990,"user":{"displayName":"Elliot Tan","photoUrl":"","userId":"13442172689223781275"}},"outputId":"bd8dac84-c930-41be-f574-0b49cd938c56"},"source":["! pip install pyspellchecker\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting pyspellchecker\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/c7/435f49c0ac6bec031d1aba4daf94dc21dc08a9db329692cdb77faac51cea/pyspellchecker-0.6.2-py3-none-any.whl (2.7MB)\n","\u001b[K     |████████████████████████████████| 2.7MB 6.7MB/s \n","\u001b[?25hInstalling collected packages: pyspellchecker\n","Successfully installed pyspellchecker-0.6.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gOBvTCx-8YyF"},"source":["#imports\n","from spellchecker import SpellChecker"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HaSLn3H28dQ_","executionInfo":{"status":"ok","timestamp":1617297360304,"user_tz":300,"elapsed":280,"user":{"displayName":"Elliot Tan","photoUrl":"","userId":"13442172689223781275"}},"outputId":"0bd88dc6-87c6-4aa0-c185-0b5d364d0da0"},"source":["spell = SpellChecker()\n","# find those words that may be misspelled\n","misspelled = [\"dawg\", \"world\"]\n","\n","for word in misspelled:\n","  print(word)\n","  print(spell.correction(word))\n","    # Get the one `most likely` answer\n","  if (spell.correction(word) == None):\n","    print(\"empty\")\n","\n","  # Get a list of `likely` options\n","  print(spell.candidates(word))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["dawg\n","dawg\n","{'dawg'}\n","world\n","world\n","{'world'}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sDI41Jq3JOld"},"source":["def correct_typos(df):\n","  spell = SpellChecker()\n","  ret = [] # return to df\n","  # get list of sentences\n","\n","  # for each row in tweet\n","  for i in range(len(df['tweet'])):\n","    # tokenize the row\n","    toks = (df['tweet'][i]).split()\n","    # for each token\n","    res_toks = []\n","    for tok in toks:\n","      if ignore_word(tok):\n","        continue\n","      # run spell checker\n","      # replace word with spell checker word\n","      correction_tok = spell.correction(tok)\n","      res_toks.append(correction_tok)\n","    # join the toks\n","    ret_str = \" \".join(res_toks)\n","    df['tweet'][i] = ret_str\n","\n","\n","  \n","  \n","  \n","\n","  return df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LwapFfD6Zp7E","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617297532704,"user_tz":300,"elapsed":16310,"user":{"displayName":"Elliot Tan","photoUrl":"","userId":"13442172689223781275"}},"outputId":"f56e8b5b-a596-4865-a0e7-e150b3bf3ddb"},"source":["test_df = df.head(10)\n","print(test_df)\n","print(\"\\n\")\n","new_df = correct_typos(test_df)\n","print(new_df)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["   Unnamed: 0  count  ...  class                                              tweet\n","0           0      3  ...      2  !!! it As a woman you shouldn't complain about...\n","1           1      3  ...      1  !!!!! it boy dats cold...tyga down bad for cof...\n","2           2      3  ...      1  !!!!!!! it Dawg!!!! it You ever fuck a bitch a...\n","3           3      3  ...      1                !!!!!!!!! it she look like a tranny\n","4           4      6  ...      1  !!!!!!!!!!!!! it The shit you hear about me mi...\n","5           5      3  ...      1  !!!!!!!!!!!!!!!!!!\"@T_Madison_x: The shit just...\n","6           6      3  ...      1  !!!!!!\"@__BrighterDays: I can not just sit up ...\n","7           7      3  ...      1  !!!!&#8220;@selfiequeenbri: cause I'm tired of...\n","8           8      3  ...      1  \" &amp; you might not get ya bitch back &amp; ...\n","9           9      3  ...      1  \" @rhythmixx_ :hobbies include: fighting Maria...\n","\n","[10 rows x 7 columns]\n","\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  after removing the cwd from sys.path.\n"],"name":"stderr"},{"output_type":"stream","text":["   Unnamed: 0  count  ...  class                                              tweet\n","0           0      3  ...      2  !!! it As a woman you shouldn't complain about...\n","1           1      3  ...      1  !!!!! it boy dats cold...tyga down bad for cof...\n","2           2      3  ...      1  !!!!!!! it Dawg!!!! it You ever fuck a bitch a...\n","3           3      3  ...      1                !!!!!!!!! it she look like a tranny\n","4           4      6  ...      1  !!!!!!!!!!!!! it The shit you hear about me mi...\n","5           5      3  ...      1  !!!!!!!!!!!!!!!!!!\"@T_Madison_x: The shit just...\n","6           6      3  ...      1  !!!!!!\"@__BrighterDays: I can not just sit up ...\n","7           7      3  ...      1  !!!!&#8220;@selfiequeenbri: cause I'm tired of...\n","8           8      3  ...      1  \" camp you might not get ya bitch back camp th...\n","9           9      3  ...      1             \" hobbies include fighting maria bitch\n","\n","[10 rows x 7 columns]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gC2MMKxhfzqU"},"source":["# Expanding abbreviations\n","### Elliot"]},{"cell_type":"code","metadata":{"id":"L3hyImT-f9Ux","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617297866068,"user_tz":300,"elapsed":24002,"user":{"displayName":"Elliot Tan","photoUrl":"","userId":"13442172689223781275"}},"outputId":"95ac78ec-f556-40c9-fe12-cff519e78671"},"source":["! pip install scispacy"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting scispacy\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/d2/456e1f66f7ba65209746aac666b22e0c11e9aee6d9f549a2fdba5d49247b/scispacy-0.4.0-py3-none-any.whl (44kB)\n","\u001b[K     |████████████████████████████████| 51kB 2.7MB/s \n","\u001b[?25hCollecting spacy<3.1.0,>=3.0.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/70/a0b8bd0cb54d8739ba4d6fb3458785c3b9b812b7fbe93b0f10beb1a53ada/spacy-3.0.5-cp37-cp37m-manylinux2014_x86_64.whl (12.8MB)\n","\u001b[K     |████████████████████████████████| 12.8MB 269kB/s \n","\u001b[?25hRequirement already satisfied: scikit-learn>=0.20.3 in /usr/local/lib/python3.7/dist-packages (from scispacy) (0.22.2.post1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from scispacy) (1.0.1)\n","Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scispacy) (2.23.0)\n","Collecting conllu\n","  Downloading https://files.pythonhosted.org/packages/ae/be/be6959c3ff2dbfdd87de4be0ccdff577835b5d08b1d25bf7fd4aaf0d7add/conllu-4.4-py2.py3-none-any.whl\n","Collecting pysbd\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/0a/c99fb7d7e176f8b176ef19704a32e6a9c6aafdf19ef75a187f701fc15801/pysbd-0.3.4-py3-none-any.whl (71kB)\n","\u001b[K     |████████████████████████████████| 71kB 6.7MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from scispacy) (1.19.5)\n","Collecting nmslib>=1.7.3.6\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/be/77/aebbd03a32488024d2ae2230b47a28f6fa83c887318e673fa5d3234f7772/nmslib-2.1.1-cp37-cp37m-manylinux2010_x86_64.whl (13.5MB)\n","\u001b[K     |████████████████████████████████| 13.5MB 29.6MB/s \n","\u001b[?25hRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->scispacy) (1.0.5)\n","Collecting thinc<8.1.0,>=8.0.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/08/20e707519bcded1a0caa6fd024b767ac79e4e5d0fb92266bb7dcf735e338/thinc-8.0.2-cp37-cp37m-manylinux2014_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 38.0MB/s \n","\u001b[?25hRequirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->scispacy) (0.8.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->scispacy) (20.9)\n","Collecting spacy-legacy<3.1.0,>=3.0.0\n","  Downloading https://files.pythonhosted.org/packages/78/d8/e25bc7f99877de34def57d36769f0cce4e895b374cdc766718efc724f9ac/spacy_legacy-3.0.2-py2.py3-none-any.whl\n","Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->scispacy) (3.7.4.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->scispacy) (2.11.3)\n","Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->scispacy) (0.4.1)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->scispacy) (2.0.5)\n","Collecting pydantic<1.8.0,>=1.7.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b3/0a/52ae1c659fc08f13dd7c0ae07b88e4f807ad83fb9954a59b0b0a3d1a8ab6/pydantic-1.7.3-cp37-cp37m-manylinux2014_x86_64.whl (9.1MB)\n","\u001b[K     |████████████████████████████████| 9.1MB 37.8MB/s \n","\u001b[?25hCollecting catalogue<2.1.0,>=2.0.1\n","  Downloading https://files.pythonhosted.org/packages/48/5c/493a2f3bb0eac17b1d48129ecfd251f0520b6c89493e9fd0522f534a9e4a/catalogue-2.0.1-py3-none-any.whl\n","Collecting pathy>=0.3.5\n","  Downloading https://files.pythonhosted.org/packages/a2/53/97dc0197cca9357369b3b71bf300896cf2d3604fa60ffaaf5cbc277de7de/pathy-0.4.0-py3-none-any.whl\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->scispacy) (4.41.1)\n","Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->scispacy) (3.8.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->scispacy) (54.2.0)\n","Collecting typer<0.4.0,>=0.3.0\n","  Downloading https://files.pythonhosted.org/packages/90/34/d138832f6945432c638f32137e6c79a3b682f06a63c488dcfaca6b166c64/typer-0.3.2-py3-none-any.whl\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->scispacy) (3.0.5)\n","Collecting srsly<3.0.0,>=2.4.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/54/76982427ceb495dd19ff982c966708c624b85e03c45bf1912feaf60c7b2d/srsly-2.4.0-cp37-cp37m-manylinux2014_x86_64.whl (456kB)\n","\u001b[K     |████████████████████████████████| 460kB 36.2MB/s \n","\u001b[?25hRequirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.3->scispacy) (1.4.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->scispacy) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->scispacy) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->scispacy) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->scispacy) (2020.12.5)\n","Collecting pybind11<2.6.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/84/fc9dc13ee536ba5e6b8fd10ce368fea5b738fe394c3b296cde7c9b144a92/pybind11-2.6.1-py2.py3-none-any.whl (188kB)\n","\u001b[K     |████████████████████████████████| 194kB 32.5MB/s \n","\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from nmslib>=1.7.3.6->scispacy) (5.4.8)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->scispacy) (2.4.7)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.1.0,>=3.0.0->scispacy) (1.1.1)\n","Collecting smart-open<4.0.0,>=2.2.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/9a/ba2d5f67f25e8d5bbf2fcec7a99b1e38428e83cb715f64dd179ca43a11bb/smart_open-3.0.0.tar.gz (113kB)\n","\u001b[K     |████████████████████████████████| 122kB 37.8MB/s \n","\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->spacy<3.1.0,>=3.0.0->scispacy) (3.4.1)\n","Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->scispacy) (7.1.2)\n","Building wheels for collected packages: smart-open\n","  Building wheel for smart-open (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for smart-open: filename=smart_open-3.0.0-cp37-none-any.whl size=107098 sha256=40d8ed0789af32dfb3b219b25887098562b55a07a5335338f2c8b1b10ddd3612\n","  Stored in directory: /root/.cache/pip/wheels/18/88/7c/f06dabd5e9cabe02d2269167bcacbbf9b47d0c0ff7d6ebcb78\n","Successfully built smart-open\n","Installing collected packages: catalogue, srsly, pydantic, thinc, spacy-legacy, typer, smart-open, pathy, spacy, conllu, pysbd, pybind11, nmslib, scispacy\n","  Found existing installation: catalogue 1.0.0\n","    Uninstalling catalogue-1.0.0:\n","      Successfully uninstalled catalogue-1.0.0\n","  Found existing installation: srsly 1.0.5\n","    Uninstalling srsly-1.0.5:\n","      Successfully uninstalled srsly-1.0.5\n","  Found existing installation: thinc 7.4.0\n","    Uninstalling thinc-7.4.0:\n","      Successfully uninstalled thinc-7.4.0\n","  Found existing installation: smart-open 4.2.0\n","    Uninstalling smart-open-4.2.0:\n","      Successfully uninstalled smart-open-4.2.0\n","  Found existing installation: spacy 2.2.4\n","    Uninstalling spacy-2.2.4:\n","      Successfully uninstalled spacy-2.2.4\n","Successfully installed catalogue-2.0.1 conllu-4.4 nmslib-2.1.1 pathy-0.4.0 pybind11-2.6.1 pydantic-1.7.3 pysbd-0.3.4 scispacy-0.4.0 smart-open-3.0.0 spacy-3.0.5 spacy-legacy-3.0.2 srsly-2.4.0 thinc-8.0.2 typer-0.3.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"q8GUt1G-iarB"},"source":["import spacy\n","import scispacy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":358},"id":"SJ7o48fRidPA","executionInfo":{"status":"error","timestamp":1617299164520,"user_tz":300,"elapsed":1205,"user":{"displayName":"Elliot Tan","photoUrl":"","userId":"13442172689223781275"}},"outputId":"df959627-1b7a-4f8a-96da-7742f712233a"},"source":["from scispacy.linking import EntityLinker\n","\n","nlp = spacy.load(\"en_core_sci_sm\")\n","nlp.add_pipe(\"scispacy_linker\", config={\"resolve_abbreviations\": True, \"linker_name\": \"umls\"})"],"execution_count":null,"outputs":[{"output_type":"error","ename":"OSError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-a3b60890c4a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscispacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinking\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEntityLinker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"en_core_sci_sm\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"scispacy_linker\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"resolve_abbreviations\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"linker_name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"umls\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, disable, exclude, config)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mRETURNS\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mLanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mloaded\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \"\"\"\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE941\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en_core_sci_sm'. It doesn't seem to be a Python package or a valid path to a data directory."]}]},{"cell_type":"markdown","metadata":{"id":"GhMptUy3ZDXL"},"source":["# Remove blank spaces/punctuations/accented characters/contractions\n","Anjali"]},{"cell_type":"code","metadata":{"id":"dGYXEMhjZIS-"},"source":["from google.colab import files\n","import pandas as pd\n","import io\n","import re\n","import unidecode\n","import contractions\n","import pandas as pd\n","# uploaded = files.upload()\n","df = pd.read_csv(\"labeled_data.csv\")\n","\n","# We use class and tweet fields from input dataset\n","dt_trasformed = df[['class', 'tweet']]\n","y = dt_trasformed.iloc[:, :-1].values\n","\n","# Preprocess the data\n","corpusTweet = []\n","corpusClass = []\n","for i in range(0, 24783):\n","  text = re.sub(\"[^0-9A-Za-z]+\", \" \", dt_trasformed['tweet'][i]) \n","  text = re.sub(\"[$]\",\" dollar \", dt_trasformed['tweet'][i])\n","  text = re.sub(\"[\\^]\",\" carat \", dt_trasformed['tweet'][i])\n","  text = re.sub(\"[%]\",\" percent \", dt_trasformed['tweet'][i])\n","  text = contractions.fix(dt_trasformed['tweet'][i])\n","  text = unidecode.unidecode(dt_trasformed['tweet'][i]) \n","  text = ''.join(text)\n","  label = dt_trasformed['class'][i]\n","  corpusTweet.append(text)\n","  corpusClass.append(label)\n","\n","# Add class and tweet to a new list\n","updatedSet = list(zip(corpusTweet, corpusClass))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1zLkbXCeZJES"},"source":["# Load GPT2 Model\n","Anjali"]},{"cell_type":"code","metadata":{"id":"0k84-gxVZL4p","colab":{"base_uri":"https://localhost:8080/","height":249},"executionInfo":{"status":"error","timestamp":1617905378424,"user_tz":300,"elapsed":167,"user":{"displayName":"Alexis Edwards","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiBybh3MHhSTM4QYO0umj0CWSJNLLaIbispEle2=s64","userId":"05307814282650774487"}},"outputId":"42e5f19d-116f-40cd-e702-e139a1004c9e"},"source":["# Use GPT2 to autogenerate text using the text from list\n","# Load Model\n","tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-large\")\n","model = GPT2LMHeadModel.from_pretrained(\"gpt2-large\", pad_token_id = tokenizer.eos_token_id)\n","tokenizer.decode(tokenizer.eos_token_id)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-42fa2798961f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Use GPT2 to autogenerate text using the text from list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Load Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPT2Tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gpt2-large\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPT2LMHeadModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gpt2-large\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_token_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_token_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_token_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'GPT2Tokenizer' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"KcN4AOMmZf6M"},"source":["# Generate the data using updated dataset and GPT2 model"]},{"cell_type":"code","metadata":{"id":"EvW7y6NVZgc1","colab":{"base_uri":"https://localhost:8080/","height":232},"executionInfo":{"status":"error","timestamp":1617905347980,"user_tz":300,"elapsed":109,"user":{"displayName":"Alexis Edwards","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiBybh3MHhSTM4QYO0umj0CWSJNLLaIbispEle2=s64","userId":"05307814282650774487"}},"outputId":"bfb356d5-9a93-49f3-8eb6-539cb331113f"},"source":["generatedTextCorpus = []\n","generatedLabelsCorpus = []\n","for tweet in range(0,24783):\n","  tweetValue = updatedSet[tweet][0]\n","  tweetLabel = updatedSet[tweet][1]\n","  encodedTweet = tokenizer.encode(tweetValue, return_tensors='pt')\n","  generatedText = model.generate(encodedTweet, max_length=100, num_beams=5, no_repeat_ngram_size=2, early_stopping=True)\n","  decodedText = tokenizer.decode(generatedText[0], skip_special_tokens=True)\n","  generatedTextCorpus.append(generatedText)\n","  generatedLabelsCorpus.append(tweetLabel)\n","\n","generatedDataSet = list(zip(generatedTextCorpus, generatedLabelsCorpus))\n","\n","generatedDataSet"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-188f1d1e9681>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mgeneratedLabelsCorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m24783\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mtweetValue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdatedSet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mtweetLabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdatedSet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mencodedTweet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweetValue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'updatedSet' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"oCsSTabgaLgw"},"source":["# Create CSV with new generated dataset\n","Anjali"]},{"cell_type":"code","metadata":{"id":"zfPSPfC4aRnC"},"source":["columns = ['text','label']\n","fileName = \"generatedDataset.csv\"\n","# Create new dataset using generated text\n","with open(filename, 'w') as csvfile: \n","  csvwriter = csv.writer(csvfile) \n","  csvwriter.writerow(columns) \n","  csvwriter.writerows(generatedDataSet)"],"execution_count":null,"outputs":[]}]}